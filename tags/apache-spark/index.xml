<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devkuma – Apache Spark</title>
    <link>https://www.devkuma.com/tags/apache-spark/</link>
    <image>
      <url>https://www.devkuma.com/tags/apache-spark/logo/180x180.jpg</url>
      <title>Apache Spark</title>
      <link>https://www.devkuma.com/tags/apache-spark/</link>
    </image>
    <description>Recent content in Apache Spark on devkuma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <managingEditor>redfreek2c@gmail.com (kimkc)</managingEditor>
    <webMaster>redfreek2c@gmail.com (kimkc)</webMaster>
    <copyright>The devkuma</copyright>
    
	  <atom:link href="https://www.devkuma.com/tags/apache-spark/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Apache Spark</title>
      <link>https://www.devkuma.com/docs/apache-spark/</link>
      <pubDate>Sat, 24 Dec 2022 10:22:54 +0900</pubDate>
      <author>redfreek2c@gmail.com (kimkc)</author>
      <guid>https://www.devkuma.com/docs/apache-spark/</guid>
      <description>
        
        
        &lt;h2 id=&#34;apache-spark-개요&#34;&gt;Apache Spark 개요&lt;/h2&gt;
&lt;p&gt;Apache Spark는 빅 데이터 및 머신 러닝과 같은 대규모 데이터를 처리하는 &lt;strong&gt;클러스터 컴퓨팅을 위한 분산 처리 프레임워크&lt;/strong&gt;이다. Spark는 2009년에 칼포르니아 버클리 대학교의 AMPLab로, Hadoop의 커미터(소스코드 입력 권한 보유한 원천개발자)이기도 한 마테이 자하리아(Mate Zaharia)에 의해 개발이 개시되어 현재는 Apache 소프트웨어 재단의 톱 프로젝트의 하나로서 관리, 개발이 계속되고 있다.&lt;/p&gt;
&lt;p&gt;Spark가 개발된 목적으로는 기존 MapReduce의 처리 속도가 느린 것에 대한 개선과 Map과 Reduce의 반복을 반복하는 스타일에 얽매이지 않는 유연한 처리 스타일에 대한 대응을 들 수 있다.&lt;/p&gt;
&lt;p&gt;Spark는 분산 처리 프레임워크로서 단독으로도 동작하기 때문에 포스트 Hadoop로서 주목받고 있는 한편, MapReduce, HDFS, YARN 등으로 이루어지는 Hadoop 코어 시스템중의 MapReduce의 대체로서 이용하는 것도 가능하게 되어 있다.&lt;/p&gt;
&lt;h2 id=&#34;apache-spark의-주요-특징&#34;&gt;Apache Spark의 주요 특징&lt;/h2&gt;
&lt;p&gt;Spark의 큰 특징으로서 Spark가 제공하는 간소한 API를 사용하여 유연한 처리 모델을 쉽게 프로그래밍 할 수 있는 것, 대규모 데이터에 대해 기존의 MapReduce에 비해 훨씬 짧은 시간에 처리를 할 수 있는 것 등이 언급할 수 있다.&lt;/p&gt;
&lt;p&gt;기존의 MapReduce에서는 처리 모델로서 Map과 Reduce를 1세트로 수행할 필요가 있었기 때문에, Hadoop상에서 움직이는 어플리케이션을 개발하기 위해서는 이 스타일에 따라서 개발할 필요가 있었다. 그 때문에 유연한 처리 모델을 개발하는 것이 어렵다고 하는 문제가 존재했었다.&lt;/p&gt;
&lt;p&gt;또한 MapReduce에서는 한 번의 Map 및 Reduce 처리마다 처리 결과를 Disk에 쓰는 것이 발생하여 처리 속도를 향상시키는 것이 어렵다는 문제가 존재했다. 이에 대해 Spark에서는 메모리에 읽어들인 데이터 세트(RDD)에 대해서 복수회의 Map 처리를 연속하여 실행하여 한층 더 Reduce한 결과를 Disk에 기입하지 않고 인메모리 상태로 한층 더 다음의 Map 처리를 그 데이터 세트에 대해서 행하는 방법을 취하고 있기 때문에, MapReduce에 비해 최대로 100배 이상의 처리 속도의 향상을 볼 수 있는 경우도 있다고 한다(기존의 MapReduce와 같이 처리 결과를 디스크에 쓸 수도 있다).&lt;/p&gt;
&lt;p&gt;스파크의 특징은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Speed
&lt;ul&gt;
&lt;li&gt;인메모리(In-Memory) 기반의 빠른 처리&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ease of Use
&lt;ul&gt;
&lt;li&gt;다양한 언어 지원(Java, Scala, Python, R, SQL)을 통한 사용의 편이성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generality
&lt;ul&gt;
&lt;li&gt;SQL, Streaming, 머신러닝, 그래프 연산 등 다양한 컴포넌트 제공&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run Everywhere
&lt;ul&gt;
&lt;li&gt;YARN, Mesos, Kubernetes 등 다양한 클러스터에서 동작 가능&lt;/li&gt;
&lt;li&gt;HDFS, Casandra, HBase 등 다양한 파일 포맷 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark의-컴포넌트-구성&#34;&gt;Apache Spark의 컴포넌트 구성&lt;/h2&gt;
&lt;p&gt;Spark는 분산 처리 프레임워크로 다음 컴포넌트 요소로 구성된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark Core (Scala, Java, Python, RAPI 포함)&lt;/li&gt;
&lt;li&gt;Spark SQL + DataFrames&lt;/li&gt;
&lt;li&gt;Spark Streaming&lt;/li&gt;
&lt;li&gt;MLlib&lt;/li&gt;
&lt;li&gt;GraphX&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;spark-core&#34;&gt;Spark Core&lt;/h3&gt;
&lt;p&gt;Spark는 처리할 데이터를 RDD(Resilient Distributed Dataset) 형식으로 유지한다.&lt;br&gt;
RDD는 변하지 않고 병렬로 실행 가능한 컬렉션으로, 분산된 각 컴퓨터에 배치된다.&lt;br&gt;
Spark를 사용한 프로그래밍 모델에서는 이 RDD에 대해서 Spark Core에서 제공되고 있는 각종 메소드를 적응하면서 처리를 실시해 간다. Spark Core에서 제공하는 API를 통해 RDD를 조작할 때 개발자는 분산된 데이터를 의식하지 않고 분산 처리를 실행할 수 있다.&lt;br&gt;
이는 Spark의 특징의 하나의 유연한 처리를 용이하게 프로그래밍 할 수 있다고 하는 점이다.&lt;br&gt;
Spark Core에서 제공되는 API이지만, Spark의 개발 언어인 Scala 이외에도 Java, Python, R이라고 말한 언어로부터 호출 가능한 API가 표준으로 제공되고 있다. 또, 3rd-Party 라이브러리 중에는 Scala와 같이 JavaVM상에서 움직이는 함수형 언어의 Clojure로부터 Spark API를 호출하기 위한 라이브러리도 존재하고 있어, 또 다른 언어의 Spark API도 계속 늘어 날거라 예상된다.&lt;/p&gt;
&lt;h3 id=&#34;spark-sql--dataframes&#34;&gt;Spark SQL + DataFrames&lt;/h3&gt;
&lt;p&gt;Spark에서는 Spark에서 제공하는 API를 통해 RDD를 조작하는 방법 외에도 Spark SQL이라는 SQL과 같은 언어를 사용하여 DataFrames라는 데이터베이스 테이블과 같은 이름 열을 가진 추상화 된 데이터 세트를 조작 할 수 있다.&lt;br&gt;
이것은 Scala, Java, Python, R과 같은 언어를 습득하지 않은 사용자라도 SQL에 대한 지식이 있으면 Spark SQL을 통해 Spark를 사용하여 데이터를 처리 할 수있는 인터페이스이다.&lt;/p&gt;
&lt;h3 id=&#34;spark-streaming&#34;&gt;Spark Streaming&lt;/h3&gt;
&lt;p&gt;Spark에서 끊임없이 전송되는 Streaming 데이터에 실시간 분산 처리를 가능하게 하는 기능을 제공하는 엔진이다.&lt;br&gt;
마찬가지로 스트리밍 데이터를 처리하는 프레임 워크로 Apache Storm이 있다. Apache Storm이 스트리밍 데이터 처리에 특화된 프레임워크인 반면 Spark Streaming은 Spark에서 실시간 데이터를 처리하는 엔진으로서의 기능이다.&lt;br&gt;
그 외에도 Apache Flink라는 스트리밍 처리 프레임워크도 있다. 이쪽은 배치 처리도 가능한 데다, 기계 학습 라이브러리나 그래프 처리 라이브러리등도 존재하기 때문에, Spark에 꽤 닮은 구성으로 되어 있어, Spark의 대항마로 불리고 있다.&lt;/p&gt;
&lt;p&gt;Apache Storm&lt;br&gt;
&lt;a href=&#34;http://storm.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://storm.apache.org/&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Apache Flink&lt;br&gt;
&lt;a href=&#34;http://flink.apache.org/introduction.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://flink.apache.org/introduction.html&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;mllib&#34;&gt;MLlib&lt;/h3&gt;
&lt;p&gt;Spark의 기계 학습 라이브러리이다. Spark의 유연한 처리 스타일로 기계 학습을 수행하는 프로그램을 작성할 수 있다.&lt;br&gt;
기계 학습 라이브러리로는 먼저 Hadoop과 연계하여 기계 학습을 수행하는 Mahout이라는 소프트웨어가 존재했지만, Hadoop + Mahout에서는 MapReduce의 프로그래밍 모델을 사용하여 기계 학습 프로그램을 만들어야 만  하 때문에 처리 속도의 저하라는 문제가 있었다.&lt;br&gt;
그에 비해 Spark는 Hadoop보다 고속으로 처리를 할 수 있어 Spark와 Spark가 제공하는 MLlib를 사용한 기계 학습이 효율이 좋다는 점에서 주목이 높아지고 있다.&lt;/p&gt;
&lt;p&gt;Apache Mahout&lt;br&gt;
&lt;a href=&#34;http://mahout.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://mahout.apache.org/&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;graphx&#34;&gt;GraphX&lt;/h3&gt;
&lt;p&gt;GraphX는 Spark를 통해 그래프 데이터를 병렬 처리하는 API를 제공한다.&lt;br&gt;
Spark의 특징인 고속 처리 속도로 그래프 데이터의 병렬 처리가 가능하다.&lt;/p&gt;
&lt;p&gt;위에서 설명한 Spark를 형성하는 일부 컴포넌트에는 스토리지로 분류되는 것이 없다.
Spark에서는 읽기 쓰기로 기존의 각종 스토리지를 이용할 수 있다. 아래는 Spark와 연동 가능한 스토리지의 일부이다. (3rd-party 라이브러리 이용 포함)&lt;/p&gt;
&lt;p&gt;HDFS, Cassandra, HBase, S3, MongoDB, Couchbase, Riak, Neo4j, OrientDB&lt;/p&gt;
&lt;p&gt;또, 읽을 수 있는 데이터 소스로서는 CSV나 XML등의 파일로부터, Solr, Elasticsearch 등의 검색 엔진등 다양하다.&lt;/p&gt;
&lt;p&gt;Spark와 각종 데이터 소스와의 연계 패키지 목록&lt;br&gt;
&lt;a href=&#34;https://spark-packages.org/?q=tags%3A%22Data%20Sources%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spark-packages.org/?q=tags%3A%22Data%20Sources%22&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;위에서 제시한 다양한 데이터 소스와의 연동을 가능하게 하는 패키지 이외에도 기존의 Spark ECO 시스템을 확장하기 위한 각종 패키지가 제공되고 있다.
이 패키지는 SparkPackage로 다음 사이트에 게시된다.&lt;br&gt;
&lt;a href=&#34;https://spark-packages.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spark-packages.org/&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;apache-spark의-운영-환경&#34;&gt;Apache Spark의 운영 환경&lt;/h2&gt;
&lt;p&gt;Spark가 동작 보증하고 있는 것은, 아래와 같은 OS가 된다. 또한 실행하려면 Java가 설치되어 있어야 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;주요 리눅스 배포판&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;리눅스&lt;/li&gt;
&lt;li&gt;MacOSX&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark에서 제공하는 API가 지원하는 버전은 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Java 8, 11, 17 (Java 8u201 미만은 Spark3.2.0에서는 더 이상 사용되지 않음)&lt;/li&gt;
&lt;li&gt;Scala 2.12, 2.13 (Spark3.3.0은 호환되는 Scala2.12.x를 사용해야 함)&lt;/li&gt;
&lt;li&gt;Python 3.7 이상 (Python3.9의 경우 Apache Arrow와 pandas UDF가 작동하지 않을 수 있음)&lt;/li&gt;
&lt;li&gt;R 3.5 이상&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark-라이센스-형식&#34;&gt;Apache Spark 라이센스 형식&lt;/h2&gt;
&lt;p&gt;Spark는 Apache의 최상위 프로젝트 중 하나이다.&lt;br&gt;
라이센스는 Apache License 2.0이며, 사용자는 소프트웨어의 사용, 배포, 수정 및 파생판 배포에 제한을 받지 않는다.&lt;/p&gt;
&lt;h2 id=&#34;apache-spark-참고-정보&#34;&gt;Apache Spark 참고 정보&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://spark.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spark 공식 사이트&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spark 공식 문서&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spark 커뮤니티 사이트&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark 개발자들이 시작한 databricks라는 회사에 의해 제공되고 있다.&lt;/p&gt;

      </description>
      
      <category>BigData</category>
      
      <category>Apache Spark</category>
      
    </item>
    
    <item>
      <title>PySpark 개념 및 주요 기능</title>
      <link>https://www.devkuma.com/docs/pyspark/</link>
      <pubDate>Fri, 06 Jan 2023 12:36:13 +0900</pubDate>
      <author>redfreek2c@gmail.com (kimkc)</author>
      <guid>https://www.devkuma.com/docs/pyspark/</guid>
      <description>
        
        
        &lt;h2 id=&#34;pyspark란&#34;&gt;PySpark란?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpark&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 실시간 대규모 데이터 처리를 위한 오픈 소스 분산 컴퓨팅 프레임워크 및 라이브러리 세트인 Apache Spark용 Python API이다. Python 및 Pandas와 같은 라이브러리에 이미 익숙하다면 PySpark는 보다 확장 가능한 분석 및 파이프라인을 만드는 방법을 배우기에 좋은 언어이다.&lt;/p&gt;
&lt;p&gt;아파치 스파크는 기본적으로 병렬 및 배치 시스템에서 처리하여 거대한 데이터 세트와 함께 작동하는 계산 엔진이다. Spark는 Scala로 작성되었으며 PySpark는 Spark와 Python의 협업을 지원하기 위해 출시되었다. Spark용 API를 제공하는 것 외에도 PySpark는 Py4j 라이브러리를 활용하여 RDD(Resilient Distributed Datasets)와의 인터페이스를 지원한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.devkuma.com/docs/apache-spark/python-spark-pyspark.png&#34; alt=&#34;Apache Spark 및 Python 로고&#34;&gt;&lt;/p&gt;
&lt;p&gt;PySpark에서 사용되는 주요 데이터 유형은 Spark 데이터 프레임이다. 이 개체는 클러스터 전체에 분산된 테이블로 생각할 수 있으며 R 및 Pandas의 데이터 프레임과 유사한 기능을 가지고 있다. PySpark를 사용하여 분산 계산을 수행하려면 다른 Python 데이터 유형이 아닌 Spark 데이터 프레임에서 작업을 수행해야 한다.&lt;/p&gt;
&lt;p&gt;Pandas와 Spark 데이터 프레임의 주요 차이점 중 하나는 즉시 실행과 지연 실행이다. PySpark에서는 결과가 파이프라인에서 실제로 요청될 때까지 작업이 지연된다. 예를 들어, Amazon S3에서 데이터 세트를 로드하고 데이터 프레임에 여러 변환을 적용하는 작업을 지정할 수 있지만 이러한 작업은 즉시 적용되지 않는다. 대신 변환 그래프가 기록되고 데이터가 실제로 필요하면(예: S3에 결과를 다시 쓸 때) 변환이 단일 파이프라인 작업으로 적용된다. 이 접근 방식은 전체 데이터 프레임을 메모리로 가져오는 것을 방지하고 시스템 클러스터 전체에서 보다 효과적인 처리를 가능하게 하는 데 사용된다. Pandas 데이터 프레임을 사용하면 모든 것을 메모리로 가져오고 모든 Pandas 작업이 즉시 적용된다.&lt;/p&gt;
&lt;h2 id=&#34;pyspark-기능-및-라이브러리&#34;&gt;PySpark 기능 및 라이브러리&lt;/h2&gt;
&lt;p&gt;Py4J는 PySpark 내에 통합되어 Python이 JVM(Java Virtual Machine) 개체와 동적으로 인터페이스할 수 있도록 하는 널리 사용되는 라이브러리이다. PySpark는 효율적인 프로그램 작성을 위한 꽤 많은 라이브러리를 제공한다. 또한 다음을 포함하여 호환되는 다양한 외부 라이브러리가 있다.&lt;/p&gt;
&lt;h3 id=&#34;pysparksql&#34;&gt;PySparkSQL&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySparkSQL&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 대용량의 구조적 또는 반구조적 데이터에 SQL과 유사한 분석을 적용하는 PySpark 라이브러리이다. PySparkSQL과 함께 SQL 쿼리를 사용할 수도 있다.&lt;/p&gt;
&lt;h3 id=&#34;mllib&#34;&gt;MLlib&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLlib&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 PySpark 및 Spark의 래퍼기계 학습(ML) 라이브러리이다. MLlib는 분류, 회귀, 클러스터링, 협업 필터링, 차원 감소 및 기본 최적화 프리미티브를 위한 많은 기계 학습 알고리즘을 지원한다.&lt;/p&gt;
&lt;h3 id=&#34;graphframes&#34;&gt;GraphFrames&lt;/h3&gt;
&lt;p&gt;(GraphFrames)[https://graphframes.github.io/graphframes/docs/_site/index.html]는 PySpark 코어 및 PySparkSQL을 사용하여 그래프 분석을 효율적으로 수행하기 위한 일련의 API를 제공하는 그래프 처리 라이브러리이다. 빠른 분산 컴퓨팅에 최적화되어 있다.&lt;/p&gt;
&lt;h2 id=&#34;마무리&#34;&gt;마무리&lt;/h2&gt;
&lt;p&gt;Python은 알고 있지만 Scala는 모르는 데이터 엔지니어에게는 PySpark가 순수 Spark 보다 훨씬 사용하기 쉽지만 단점도 있다. PySpark 오류는 Java 스택 추적 오류와 Python 코드에 대한 참조를 모두 표시하므로 PySpark 애플리케이션 디버깅이 매우 어려울 수 있다.&lt;/p&gt;
&lt;p&gt;Spark는 다른 데이터 처리 옵션보다 더 많은 처리 오버헤드와 더 복잡한 설정을 포함한다. Ray 및 Dask가 최근에 등장하였다. Dask는 순수한 Python 프레임워크이므로 대부분의 데이터 엔지니어에게는 바로  즉시 Dask를 사용할 수 있다.&lt;/p&gt;
&lt;h2 id=&#34;참조&#34;&gt;참조&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/kr/glossary/pyspark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpark – Databricks&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dominodatalab.com/data-science-dictionary/pyspark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is PySpark? | Domino Data Science Dictionary&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
      
      <category>BigData</category>
      
      <category>Apache Spark</category>
      
      <category>PySpark</category>
      
    </item>
    
  </channel>
</rss>
