<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devkuma – PySpark</title>
    <link>https://www.devkuma.com/tags/pyspark/</link>
    <image>
      <url>https://www.devkuma.com/tags/pyspark/logo/180x180.jpg</url>
      <title>PySpark</title>
      <link>https://www.devkuma.com/tags/pyspark/</link>
    </image>
    <description>Recent content in PySpark on devkuma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <managingEditor>redfreek2c@gmail.com (kimkc)</managingEditor>
    <webMaster>redfreek2c@gmail.com (kimkc)</webMaster>
    <copyright>The devkuma</copyright>
    
	  <atom:link href="https://www.devkuma.com/tags/pyspark/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>PySpark 개념 및 주요 기능</title>
      <link>https://www.devkuma.com/docs/pyspark/</link>
      <pubDate>Fri, 06 Jan 2023 12:36:13 +0900</pubDate>
      <author>redfreek2c@gmail.com (kimkc)</author>
      <guid>https://www.devkuma.com/docs/pyspark/</guid>
      <description>
        
        
        &lt;h2 id=&#34;pyspark란&#34;&gt;PySpark란?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpark&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 실시간 대규모 데이터 처리를 위한 오픈 소스 분산 컴퓨팅 프레임워크 및 라이브러리 세트인 Apache Spark용 Python API이다. Python 및 Pandas와 같은 라이브러리에 이미 익숙하다면 PySpark는 보다 확장 가능한 분석 및 파이프라인을 만드는 방법을 배우기에 좋은 언어이다.&lt;/p&gt;
&lt;p&gt;아파치 스파크는 기본적으로 병렬 및 배치 시스템에서 처리하여 거대한 데이터 세트와 함께 작동하는 계산 엔진이다. Spark는 Scala로 작성되었으며 PySpark는 Spark와 Python의 협업을 지원하기 위해 출시되었다. Spark용 API를 제공하는 것 외에도 PySpark는 Py4j 라이브러리를 활용하여 RDD(Resilient Distributed Datasets)와의 인터페이스를 지원한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.devkuma.com/docs/apache-spark/python-spark-pyspark.png&#34; alt=&#34;Apache Spark 및 Python 로고&#34;&gt;&lt;/p&gt;
&lt;p&gt;PySpark에서 사용되는 주요 데이터 유형은 Spark 데이터 프레임이다. 이 개체는 클러스터 전체에 분산된 테이블로 생각할 수 있으며 R 및 Pandas의 데이터 프레임과 유사한 기능을 가지고 있다. PySpark를 사용하여 분산 계산을 수행하려면 다른 Python 데이터 유형이 아닌 Spark 데이터 프레임에서 작업을 수행해야 한다.&lt;/p&gt;
&lt;p&gt;Pandas와 Spark 데이터 프레임의 주요 차이점 중 하나는 즉시 실행과 지연 실행이다. PySpark에서는 결과가 파이프라인에서 실제로 요청될 때까지 작업이 지연된다. 예를 들어, Amazon S3에서 데이터 세트를 로드하고 데이터 프레임에 여러 변환을 적용하는 작업을 지정할 수 있지만 이러한 작업은 즉시 적용되지 않는다. 대신 변환 그래프가 기록되고 데이터가 실제로 필요하면(예: S3에 결과를 다시 쓸 때) 변환이 단일 파이프라인 작업으로 적용된다. 이 접근 방식은 전체 데이터 프레임을 메모리로 가져오는 것을 방지하고 시스템 클러스터 전체에서 보다 효과적인 처리를 가능하게 하는 데 사용된다. Pandas 데이터 프레임을 사용하면 모든 것을 메모리로 가져오고 모든 Pandas 작업이 즉시 적용된다.&lt;/p&gt;
&lt;h2 id=&#34;pyspark-기능-및-라이브러리&#34;&gt;PySpark 기능 및 라이브러리&lt;/h2&gt;
&lt;p&gt;Py4J는 PySpark 내에 통합되어 Python이 JVM(Java Virtual Machine) 개체와 동적으로 인터페이스할 수 있도록 하는 널리 사용되는 라이브러리이다. PySpark는 효율적인 프로그램 작성을 위한 꽤 많은 라이브러리를 제공한다. 또한 다음을 포함하여 호환되는 다양한 외부 라이브러리가 있다.&lt;/p&gt;
&lt;h3 id=&#34;pysparksql&#34;&gt;PySparkSQL&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySparkSQL&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 대용량의 구조적 또는 반구조적 데이터에 SQL과 유사한 분석을 적용하는 PySpark 라이브러리이다. PySparkSQL과 함께 SQL 쿼리를 사용할 수도 있다.&lt;/p&gt;
&lt;h3 id=&#34;mllib&#34;&gt;MLlib&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLlib&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;는 PySpark 및 Spark의 래퍼기계 학습(ML) 라이브러리이다. MLlib는 분류, 회귀, 클러스터링, 협업 필터링, 차원 감소 및 기본 최적화 프리미티브를 위한 많은 기계 학습 알고리즘을 지원한다.&lt;/p&gt;
&lt;h3 id=&#34;graphframes&#34;&gt;GraphFrames&lt;/h3&gt;
&lt;p&gt;(GraphFrames)[https://graphframes.github.io/graphframes/docs/_site/index.html]는 PySpark 코어 및 PySparkSQL을 사용하여 그래프 분석을 효율적으로 수행하기 위한 일련의 API를 제공하는 그래프 처리 라이브러리이다. 빠른 분산 컴퓨팅에 최적화되어 있다.&lt;/p&gt;
&lt;h2 id=&#34;마무리&#34;&gt;마무리&lt;/h2&gt;
&lt;p&gt;Python은 알고 있지만 Scala는 모르는 데이터 엔지니어에게는 PySpark가 순수 Spark 보다 훨씬 사용하기 쉽지만 단점도 있다. PySpark 오류는 Java 스택 추적 오류와 Python 코드에 대한 참조를 모두 표시하므로 PySpark 애플리케이션 디버깅이 매우 어려울 수 있다.&lt;/p&gt;
&lt;p&gt;Spark는 다른 데이터 처리 옵션보다 더 많은 처리 오버헤드와 더 복잡한 설정을 포함한다. Ray 및 Dask가 최근에 등장하였다. Dask는 순수한 Python 프레임워크이므로 대부분의 데이터 엔지니어에게는 바로  즉시 Dask를 사용할 수 있다.&lt;/p&gt;
&lt;h2 id=&#34;참조&#34;&gt;참조&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/kr/glossary/pyspark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpark – Databricks&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dominodatalab.com/data-science-dictionary/pyspark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is PySpark? | Domino Data Science Dictionary&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
      
      <category>BigData</category>
      
      <category>Apache Spark</category>
      
      <category>PySpark</category>
      
    </item>
    
  </channel>
</rss>
