<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devkuma – RAG</title>
    <link>https://www.devkuma.com/tags/rag/</link>
    <image>
      <url>https://www.devkuma.com/tags/rag/logo/180x180.jpg</url>
      <title>RAG</title>
      <link>https://www.devkuma.com/tags/rag/</link>
    </image>
    <description>Recent content in RAG on devkuma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <managingEditor>kc@example.com (kc kim)</managingEditor>
    <webMaster>kc@example.com (kc kim)</webMaster>
    <copyright>The devkuma</copyright>
    
	  <atom:link href="https://www.devkuma.com/tags/rag/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>RAG(Retrieval-Augmented Generation)</title>
      <link>https://www.devkuma.com/docs/ai/rag/</link>
      <pubDate>Sat, 30 Aug 2025 13:09:00 +0900</pubDate>
      <author>kc@example.com (kc kim)</author>
      <guid>https://www.devkuma.com/docs/ai/rag/</guid>
      <description>
        
        
        &lt;h2 id=&#34;rag-retrieval-augmented-generation-개념&#34;&gt;RAG (Retrieval-Augmented Generation) 개념&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RAG = 검색(Retrieval) + 생성(Generation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;LLM(대규모 언어모델)이 자기 내부 지식만으로 답을 생성하는 것이 아니라, 외부 데이터베이스(예: 문서, 벡터 DB, 위키, 사내 자료 등)에서 관련 정보를 검색한 후, 그 결과를 바탕으로 답변을 생성한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;즉, 단순히 &amp;ldquo;모델이 아는 것&amp;quot;만 쓰는 게 아니라, “필요할 때 외부에서 찾아보고 답하는” 똑똑한 비서 같은 개념이다&lt;/p&gt;
&lt;h2 id=&#34;왜-필요한가&#34;&gt;왜 필요한가?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LLM의 지식 한계 극복
&lt;ul&gt;
&lt;li&gt;LLM은 학습 시점 이후의 최신 정보를 알지 못한다.&lt;/li&gt;
&lt;li&gt;예를 들어, GPT 같은 모델은 학습 시점 이후의 최신 정보는 모른다.&lt;/li&gt;
&lt;li&gt;RAG를 사용하면 DB/웹에서 찾아온 자료를 활용 가능하게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;환각(Hallucination) 줄이기
&lt;ul&gt;
&lt;li&gt;LLM은 모르는 것도 지어낼 때가 있다.&lt;/li&gt;
&lt;li&gt;외부 근거 자료를 활용하면 답변 신뢰도를 높일 수 있다.&lt;/li&gt;
&lt;li&gt;근거 없는 답변 대신, 실제 문서/DB를 근거로 답변 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;맞춤형 지식 활용
&lt;ul&gt;
&lt;li&gt;기업 내부 문서, 보고서, 고객 FAQ, 논문, 코드베이스 등의 &lt;strong&gt;전용 데이터&lt;/strong&gt;를 LLM이 사용할 수 있음.&lt;/li&gt;
&lt;li&gt;사내 비밀 문서를 학습시키지 않고도 활용 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rag의-동작-구조&#34;&gt;RAG의 동작 구조&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;질의(Query) 입력
&lt;ul&gt;
&lt;li&gt;사용자가 질문을 입력한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;검색(Retrieval) 단계
&lt;ul&gt;
&lt;li&gt;질문을 벡터화(임베딩) 후, 벡터 데이터베이스에서 관련 문서를 검색한다.&lt;/li&gt;
&lt;li&gt;대표 DB: Pinecone, Weaviate, Milvus, FAISS 등.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;생성(Generation) 단계
&lt;ul&gt;
&lt;li&gt;LLM이 검색된 문서를 참고하여 답변을 생성하여 함께 전달한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.devkuma.com/docs/ai/rag.png&#34; alt=&#34;RAG&#34;&gt;&lt;/p&gt;
&lt;p&gt;즉, &lt;strong&gt;&amp;ldquo;찾아서 → 참고해서 → 답변하는&amp;rdquo; 구조&lt;/strong&gt;이다.&lt;/p&gt;
&lt;h2 id=&#34;예시&#34;&gt;예시&lt;/h2&gt;
&lt;p&gt;예를 들어, “우리 회사의 2023년 매출은 얼마야?”라는 질문이 들어오면:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM 단독: &amp;ldquo;2023년 매출은 1억 달러입니다.&amp;rdquo; (근거 없음, 틀릴 수 있음)&lt;/li&gt;
&lt;li&gt;RAG 활용: 회사 내부 재무 보고서를 검색 → 관련 데이터 가져옴 → &amp;ldquo;2023년 당사의 매출은 9,200억 원으로, 전년 대비 8% 성장했습니다.&amp;rdquo; (근거 있는 답변)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;비유로-이해하기&#34;&gt;비유로 이해하기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLM 단독&lt;/strong&gt;: 기억력 좋은 사람, 하지만 최신 정보는 모를 수 있다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG 사용&lt;/strong&gt;: 기억력 좋은 사람이 &lt;strong&gt;사전·검색 엔진&lt;/strong&gt;을 참고해서 답변하는 것이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rag와-fine-tuning의-비교&#34;&gt;RAG와 Fine-tuning의 비교&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fine-tuning: 모델 자체를 추가 학습 → 새로운 지식을 “내재화”&lt;/li&gt;
&lt;li&gt;RAG: 모델은 그대로 두고, 외부 자료를 검색해서 활용&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;방법&lt;/th&gt;
          &lt;th&gt;장점&lt;/th&gt;
          &lt;th&gt;단점&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Fine-tuning&lt;/td&gt;
          &lt;td&gt;응답이 빠르고 자연스러움&lt;/td&gt;
          &lt;td&gt;데이터 업데이트할 때마다 재학습 필요&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;RAG&lt;/td&gt;
          &lt;td&gt;항상 최신/맞춤 정보 반영 가능, 빠른 구축&lt;/td&gt;
          &lt;td&gt;검색 품질에 따라 답변 품질 좌우&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;실무에서는 RAG + 필요시 일부 Fine-tuning을 섞어서 많이 사용된다.&lt;/p&gt;
&lt;h2 id=&#34;rag-구현에-쓰이는-기술-스택&#34;&gt;RAG 구현에 쓰이는 기술 스택&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;임베딩 모델: OpenAI Embeddings, Sentence-BERT 등&lt;/li&gt;
&lt;li&gt;벡터 DB: Pinecone, Weaviate, Milvus, FAISS&lt;/li&gt;
&lt;li&gt;LLM: GPT, Claude, LLaMA, Gemini 등&lt;/li&gt;
&lt;li&gt;프레임워크: LangChain, LlamaIndex, Haystack&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;정리&#34;&gt;정리&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RAG는 LLM이 검색 시스템을 함께 사용해, 신뢰할 수 있고 최신 정보를 반영하는 답변을 생성하는 방식이다.&lt;/li&gt;
&lt;li&gt;즉, 지식의 확장 &amp;amp; 신뢰성 보강을 위한 핵심 기술이다.&lt;/li&gt;
&lt;/ul&gt;

      </description>
      
      <category>AI</category>
      
      <category>RAG</category>
      
    </item>
    
  </channel>
</rss>
